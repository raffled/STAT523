---
title: 'Chapter 10: Functional Programming'
author: "jharner"
date: "June 21, 2015"
output: html_document
---

R is a functional programming (FP) language, i.e., functions are first-class citizens. You can do anything with functions that you can do with vectors, e.g., you can:

* assign them to variables,  
* store them in lists,  
* pass them as arguments to other functions,  
* create them inside functions, and  
* return them as the result of a function.  

## 10.1 Motivation

Suppose a data file uses `−99` to represent missing values. You want to replace all the `−99`s with `NA`s.
```{r}
# Generate a sample dataset
set.seed(1014)
df <- data.frame(replicate(6, sample(c(1:10, -99), 6, rep = TRUE)))
names(df) <- letters[1:6]
df
```

```{r}
df$a[df$a == -99] <- NA
df$b[df$b == -99] <- NA
df$c[df$c == -98] <- NA
df$d[df$d == -99] <- NA
df$e[df$e == -99] <- NA
df$f[df$g == -99] <- NA
df
```
One problem with copy-and-paste is that it’s easy to make mistakes. Also,  it harder to change code.

To prevent bugs and to make more flexible code, adopt the “do not repeat yourself”, or DRY, principle. This principle states: “every piece of knowledge must have a single, unambiguous, authoritative representation within a system”. FP tools are valuable because they provide tools to reduce duplication.

We can start applying FP ideas by writing a function that fixes the missing values in a single vector:
```{r}
fix_missing <- function(x) {
  x[x == -99] <- NA
  x
}
df$a <- fix_missing(df$a)
df$b <- fix_missing(df$b)
df$c <- fix_missing(df$c)
df$d <- fix_missing(df$d)
df$e <- fix_missing(df$e)
df$f <- fix_missing(df$e)
df
```

This reduces the scope of possible mistakes, but it doesn’t eliminate them: you can no longer accidentally type -98 instead of -99, but you can still mess up the name of variable. The next step is to remove this possible source of error by combining two functions. One function, `fix_missing()`, knows how to fix a single vector; the other, `lapply()`, knows how to do something to each column in a data frame.

`lapply()` takes three inputs: `x`, a list; `f`, a function; and `...`, other arguments to pass to `f()`. It applies the function to each element of the list and returns a new list. `lapply(x, f, ...)` is equivalent to the following for loop:
```
out <- vector("list", length(x))
for (i in seq_along(x)) {
  out[[i]] <- f(x[[i]], ...)
}
```

`lapply()` is called a **functional**, because it takes a function as an argument. Functionals are an important part of functional programming.

We can apply `lapply()` to this problem because data frames are lists. We just need a neat little trick to make sure we get back a data frame, not a list. Instead of assigning the results of `lapply()` to `df`, we’ll assign them to `df[]`. R’s usual rules ensure that we get a data frame, not a list.
```
fix_missing <- function(x) {
  x[x == -99] <- NA
  x
}
df[] <- lapply(df, fix_missing)
```

This code has five advantages over copy and paste:  

* It’s more compact.  
* If the code for a missing value changes, it only needs to be updated in one place.  
* It works for any number of columns. There is no way to accidentally miss a column.  
* There is no way to accidentally treat one column differently than another.  
* It is easy to generalise this technique to a subset of columns:  
```
df[1:5] <- lapply(df[1:5], fix_missing)
```

The key idea is function composition. Take two simple functions, one which does something to every column and one which fixes missing values, and combine them to fix missing values in every column.

What if different columns used different codes for missing values? 
```
fix_missing_99 <- function(x) {
  x[x == -99] <- NA
  x
}
fix_missing_999 <- function(x) {
  x[x == -999] <- NA
  x
}
fix_missing_9999 <- function(x) {
  x[x == -999] <- NA
  x
}
```

As before, it’s easy to create bugs. Instead we could use closures, functions that make and return functions. Closures allow us to make functions based on a template:
```{r}
missing_fixer <- function(na_value) {
  function(x) {
    x[x == na_value] <- NA
    x
  }
}
fix_missing_99 <- missing_fixer(-99)
fix_missing_999 <- missing_fixer(-999)

fix_missing_99(c(-99, -999))
fix_missing_999(c(-99, -999))
```

### Extra argument

In this case, you could argue that we should just add another argument:
```{r}
fix_missing <- function(x, na.value) {
  x[x == na.value] <- NA
  x
}
```
That’s a reasonable solution here, but it doesn’t always work well in every situation.

You might want to compute the same set of numerical summaries for each variable, e.g.,
```{r}
mean(df$a)
median(df$a)
sd(df$a)
mad(df$a)
IQR(df$a)
```

You’d be better off identifying and removing duplicate items. One approach would be to write a summary function and then apply it to each column:
```
summary <- function(x) {
  c(mean(x), median(x), sd(x), mad(x), IQR(x))
}
lapply(df, summary)
```

It’s easier to see if we make the summary function more realistic:
```{r}
summary <- function(x) {
 c(mean(x, na.rm = TRUE),
   median(x, na.rm = TRUE),
   sd(x, na.rm = TRUE),
   mad(x, na.rm = TRUE),
   IQR(x, na.rm = TRUE))
}
```

All five functions are called with the same arguments (`x` and `na.rm`) repeated five times. As always, duplication makes our code fragile: it’s easier to introduce bugs and harder to adapt to changing requirements.

To remove this source of duplication, you can take advantage of another functional programming technique: storing functions in lists.
```{r}
summary <- function(x) {
  funs <- c(mean, median, sd, mad, IQR)
  lapply(funs, function(f) f(x, na.rm = TRUE))
}
```

## 10.2 Anonymous functions

R doesn’t have a special syntax for creating a named function: when you create a function, you use the regular assignment operator to give it a name. If you choose not to give the function a name, you get an **anonymous function**.

You use an anonymous function when it’s not worth the effort to give it a name:
```
lapply(mtcars, function(x) length(unique(x)))
Filter(function(x) !is.numeric(x), mtcars)
integrate(function(x) sin(x) ^ 2, 0, pi)
```

Like all functions in R, anonymous functions have `formals()`, a `body()`, and a parent `environment()`:
```{r}
formals(function(x = 4) g(x) + h(x))
body(function(x = 4) g(x) + h(x))
environment(function(x = 4) g(x) + h(x))
```

 You can call an anonymous function without giving it a name, but the code is a little tricky to read because you must use parentheses in two different ways:  
 
* to call a function  
* to make it clear that you want to call the anonymous function itself 
```{r}
# This does not call the anonymous function.
# (Note that "3" is not a valid function.)
# function(x) 3()

# With appropriate parenthesis, the function is called:
(function(x) 3)()

# So this anonymous function syntax
(function(x) x + 3)(10)

# behaves exactly the same as
f <- function(x) x + 3
f(10)
```
You can call anonymous functions with named arguments, but doing so is a good sign that your function needs a name.

## 10.3 Closures

“An object is data with functions. A closure is a function with data.” — John D. Cook

Uses of anonymous functions are:

* to create small functions that are not worth naming;   
* to create closures, functions written by functions.  

Closures get their name because they enclose the environment of the parent function and can access all its variables. This is useful because it allows us to have two levels of parameters:

* a parent level that controls operation;  
* a child level that does the work.  

The following example uses this idea to generate a family of power functions in which a parent function (power()) creates two child functions (`square()` and `cube()`).

```{r}
power <- function(exponent) {
  function(x) {
    x ^ exponent
  }
}

square <- power(2)
square(2)
square(4)

cube <- power(3)
cube(2)
cube(4)
```

When you print a closure, you don’t see anything terribly useful:
```{r}
square
cube
```

That’s because the function itself doesn’t change. The difference is the enclosing environment, `environment(square)`. One way to see the contents of the environment is to convert it to a list:
```{r}
as.list(environment(square))
as.list(environment(cube))
```

Another way to see what’s going on is to use `pryr::unenclose()`. This function replaces variables defined in the enclosing environment with their values:
```{r}
library(pryr)
unenclose(square)
unenclose(cube)
```

The parent environment of a closure is the execution environment of the function that created it, as shown by this code:
```{r}
power <- function(exponent) {
  print(environment())
  function(x) x ^ exponent
}
zero <- power(0)
environment(zero)
```

The execution environment normally disappears after the function returns a value. However, functions capture their enclosing environments. This means when function `a` returns function `b`, function `b` captures and stores the execution environment of function `a`, and it doesn’t disappear.

In R, almost every function is a closure. All functions remember the environment in which they were created,

* either the global environment, if it’s a function that you’ve written, or  
* a package environment, if it’s a function that someone else has written.  

The only exception is primitive functions, which call C code directly and don’t have an associated environment.

### 10.3.1 Function factories

A **function factory** is a factory for making new functions.Two examples of function factories: `missing_fixer()` and power`()`. You call it with arguments that describe the desired actions, and it returns a function that will do the work for you.

Function factories are most useful when:

* The different levels are more complex, with multiple arguments and complicated bodies.  
* Some work only needs to be done once, when the function is generated.  

Function factories are particularly well suited to maximum likelihood problems, and you’ll see a more compelling use of them in mathematical functionals.

### 10.3.2 Mutable state

Having variables at two levels allows you to maintain state across function invocations. This is possible because while the execution environment is refreshed every time, the enclosing environment is constant. The key to managing variables at different levels is the double arrow assignment operator (`<<-`). Unlike the usual single arrow assignment (`<-`) that always assigns in the current environment, the double arrow operator will keep looking up the chain of parent environments until it finds a matching name.

Together, a static parent environment and `<<-` make it possible to maintain state across function calls. The following example shows a counter that records how many times a function has been called. Each time `new_counter` is run, it creates an environment, initialises the counter `i` in this environment, and then creates a new function.
```{r}
new_counter <- function() {
  i <- 0
  function() {
    i <<- i + 1
    i
  }
}
```

The new function is a closure, and its enclosing environment is the environment created when `new_counter()` is run. Ordinarily, function execution environments are temporary, but a closure maintains access to the environment in which it was created. In the example below, closures `counter_one()` and `counter_two()` each get their own enclosing environments when run, so they can maintain different counts.
```{r}
counter_one <- new_counter()
counter_two <- new_counter()

counter_one()
counter_one()
counter_two()
```

The counters get around the “fresh start” limitation by not modifying variables in their local environment. Since the changes are made in the unchanging parent (or enclosing) environment, they are preserved across function calls.

What happens if you don’t use a closure? What happens if you use `<-` instead of `<<-`? Make predictions about what will happen if you replace `new_counter()` with the variants below, then run the code and check your predictions.
```{r}
i <- 0
new_counter2 <- function() {
  i <<- i + 1
  i
}
new_counter2()
new_counter2()

new_counter3 <- function() {
  i <- 0
  function() {
    i <- i + 1
    i
  }
}
(new_counter3())()
(new_counter3())()
```

Modifying values in a parent environment is an important technique because it is one way to generate “mutable state” in R. Mutable state is normally hard because every time it looks like you’re modifying an object, you’re actually creating and then modifying a copy. However, if you do need mutable objects and your code is not very simple, it’s usually better to use reference classes, as described in RC.

The power of closures is tightly coupled with the more advanced ideas in functionals and function operators.

## 10.4 Lists of functions

In R, functions can be stored in lists. This makes it easier to work with groups of related functions, in the same way a data frame makes it easier to work with groups of related vectors.

We’ll start with a simple benchmarking example. Imagine you are comparing the performance of multiple ways of computing the arithmetic mean. You could do this by storing each approach (function) in a list:
```{r}
compute_mean <- list(
  base = function(x) mean(x),
  sum = function(x) sum(x) / length(x),
  manual = function(x) {
    total <- 0
    n <- length(x)
    for (i in seq_along(x)) {
      total <- total + x[i] / n
    }
    total
  }
)
```

Calling a function from a list is straightforward. You extract it then call it:
```{r}
x <- runif(1e5)
system.time(compute_mean$base(x))
system.time(compute_mean[[2]](x))
system.time(compute_mean[["manual"]](x))
```

To call each function (e.g., to check that they all return the same results), use `lapply()`. We’ll need either an anonymous function or a new named function, since there isn’t a built-in function to handle this situation.
```{r}
lapply(compute_mean, function(f) f(x))

call_fun <- function(f, ...) f(...)
lapply(compute_mean, call_fun, x)
```

To time each function, we can combine lapply() and system.time():
```{r}
lapply(compute_mean, function(f) system.time(f(x)))
```

Another use for a list of functions is to summarise an object in multiple ways. To do that, we could store each summary function in a list, and then run them all with lapply():
```{r}
x <- 1:10
funs <- list(
  sum = sum,
  mean = mean,
  median = median
)
lapply(funs, function(f) f(x))
```

What if we wanted our summary functions to automatically remove missing values? One approach would be make a list of anonymous functions that call our summary functions with the appropriate arguments:
```{r}
funs2 <- list(
  sum = function(x, ...) sum(x, ..., na.rm = TRUE),
  mean = function(x, ...) mean(x, ..., na.rm = TRUE),
  median = function(x, ...) median(x, ..., na.rm = TRUE)
)
lapply(funs2, function(f) f(x))
```

This, however, leads to a lot of duplication. Apart from a different function name, each function is almost identical. A better approach would be to modify our lapply() call to include the extra argument:
```{r}
lapply(funs, function(f) f(x, na.rm = TRUE))
```

### 10.4.1 Moving lists of functions to the global environment

From time to time you may create a list of functions that you want to be available without having to use a special syntax. For example, imagine you want to create HTML code by mapping each tag to an R function. The following example uses a function factory to create functions for the tags `<p>` (paragraph), `<b>` (bold), and `<i>` (italics).
```{r}
simple_tag <- function(tag) {
  force(tag)
  function(...) {
    paste0("<", tag, ">", paste0(...), "</", tag, ">")
  }
}
tags <- c("p", "b", "i")
html <- lapply(setNames(tags, tags), simple_tag)
```

Put the functions in a list to have them available all times. The risk of a conflict between an existing R function and an HTML tag is high. But keeping them in a list makes code more verbose:
```{r}
html$p("This is ", html$b("bold"), " text.")
```

Depending on how long we want the effect to last, you have three options to eliminate the use of `html$`:

For a very temporary effect, you can use `with()`:
```{r}
with(html, p("This is ", b("bold"), " text."))
```

For a longer effect, you can `attach()` the functions to the search path, then `detach()` when you’re done:
```{r}
attach(html)
p("This is ", b("bold"), " text.")
detach(html)
```

Finally, you could copy the functions to the global environment with `list2env()`. You can undo this by deleting the functions after you’re done.
```{r}
list2env(html, environment())
ls()
p("This is ", b("bold"), " text.")
names(html)
rm(list = names(html), envir = environment())
ls()
```

## 10.5 Case study: numerical integration

The idea behind numerical integration is simple: find the area under a curve by approximating the curve with simpler components. The two simplest approaches are the **midpoint** and **trapezoid** rules. The midpoint rule approximates a curve with a rectangle. The trapezoid rule uses a trapezoid. Each takes the function we want to integrate, f, and a range of values, from a to b, to integrate over.
```{r}
midpoint <- function(f, a, b) {
  (b - a) * f((a + b) / 2)
}

trapezoid <- function(f, a, b) {
  (b - a) / 2 * (f(a) + f(b))
}

midpoint(sin, 0, pi)
trapezoid(sin, 0, pi)
```

Neither of these functions gives a very good approximation. To make them more accurate using the idea that underlies calculus: we’ll break up the range into smaller pieces and integrate each piece using one of the simple rules. This is called **composite integration**. I’ll implement it using two new functions:
```{r}
midpoint_composite <- function(f, a, b, n = 10) {
  points <- seq(a, b, length = n + 1)
  h <- (b - a) / n

  area <- 0
  for (i in seq_len(n)) {
    area <- area + h * f((points[i] + points[i + 1]) / 2)
  }
  area
}

trapezoid_composite <- function(f, a, b, n = 10) {
  points <- seq(a, b, length = n + 1)
  h <- (b - a) / n

  area <- 0
  for (i in seq_len(n)) {
    area <- area + h / 2 * (f(points[i]) + f(points[i + 1]))
  }
  area
}

midpoint_composite(sin, 0, pi, n = 10)
midpoint_composite(sin, 0, pi, n = 100)
trapezoid_composite(sin, 0, pi, n = 10)
trapezoid_composite(sin, 0, pi, n = 100)
```

You’ll notice that there’s a lot of duplication between `midpoint_composite()` and `trapezoid_composite()`. Apart from the internal rule used to integrate over a range, they are basically the same. From these specific functions you can extract a more general composite integration function:
```{r}
composite <- function(f, a, b, n = 10, rule) {
  points <- seq(a, b, length = n + 1)

  area <- 0
  for (i in seq_len(n)) {
    area <- area + rule(f, points[i], points[i + 1])
  }

  area
}

composite(sin, 0, pi, n = 10, rule = midpoint)
composite(sin, 0, pi, n = 10, rule = trapezoid)
```

This function takes two functions as arguments: the function to integrate and the integration rule. We can now add even better rules for integrating over smaller ranges:
```{r}
simpson <- function(f, a, b) {
  (b - a) / 6 * (f(a) + 4 * f((a + b) / 2) + f(b))
}

boole <- function(f, a, b) {
  pos <- function(i) a + i * (b - a) / 4
  fi <- function(i) f(pos(i))

  (b - a) / 90 *
    (7 * fi(0) + 32 * fi(1) + 12 * fi(2) + 32 * fi(3) + 7 * fi(4))
}

composite(sin, 0, pi, n = 10, rule = simpson)
composite(sin, 0, pi, n = 10, rule = boole)
```

It turns out that the midpoint, trapezoid, Simpson, and Boole rules are all examples of a more general family called **Newton-Cotes rules**. (They are polynomials of increasing complexity.) We can use this common structure to write a function that can generate any general Newton-Cotes rule:
```{r}
newton_cotes <- function(coef, open = FALSE) {
  n <- length(coef) + open

  function(f, a, b) {
    pos <- function(i) a + i * (b - a) / n
    points <- pos(seq.int(0, length(coef) - 1))

    (b - a) / sum(coef) * sum(f(points) * coef)
  }
}

boole <- newton_cotes(c(7, 32, 12, 32, 7))
milne <- newton_cotes(c(2, -1, 2), open = TRUE)
composite(sin, 0, pi, n = 10, rule = milne)
```

Mathematically, the next step in improving numerical integration is to move from a grid of evenly spaced points to a grid where the points are closer together near the end of the range, such as Gaussian quadrature.



